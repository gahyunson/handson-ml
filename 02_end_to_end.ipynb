{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 머신러닝에 대해 알아가는 장\n",
    "\n",
    "### 2.1. 문제 정의\n",
    "비즈니스의 목적을 정확히 파악하자. 회사는 우리가 만드는 모델로 어떤 이익을 얻으려할까를 알아야한다.\\\n",
    "목적은 아래 사항을 결정 짓는데 핵심역할을 한다.\n",
    "1. 문제 구성\n",
    "2. 알고리즘 선택\n",
    "3. 모델 평가 성능 지표 선택\n",
    "4. 모델 튜닝을 얼마나 시도할지\n",
    "\n",
    "파이프라인이란? \n",
    "- 데이터를 조작, 변환시키는 작업을 한다. \n",
    "- 데이터를 처리하는 component들이 연속으로 배치된 것 = 데이터 Pipeline\n",
    "\n",
    "\n",
    "여기서 컴포넌트란? \n",
    "- 비동기적 동작, 각 component는 독립적이다. \n",
    "- 각 component는 각자 데이터를 추출, 처리하고 결과는 data storage로 보낸다. \n",
    "- 구조는 아래와 같다. component사이에는 인터페이스는 data storage만 존재한다. \n",
    "- data storage|component|data storage|component|data storage|component|... \n",
    "- 장점 : 각 component는 독립적이기 때문에 각 팀은 각자의 component에 집중할 수 있다. 한 component가 다운돼도 하위 component는 한동안 잘 작동될 수 있다. (물론 다운된 component의 마지막 출력값을 이용)\n",
    "- 단점 : 모니터링이 되지 않아 고장난것을 모를 수 있고 시간이 지날수록 전체 시스템의 성능이 떨어진다.\n",
    "\n",
    "현재 솔루션이 어떻게 구성되어있는가?\n",
    "- 지도 학습 \\/ 비지도 학습 \\/ 강화 학습\n",
    "- 분류 \\/ 회귀 \\/ etc\n",
    "- 배치 학습 \\/ 온라인 학습 \n",
    "\n",
    "### 2.2 성능 측정 지표 선택\n",
    "RMSE, MAE : 예측값의 벡터 & 실제값의 벡터 사이의 거리를 재는 방법 \\ \n",
    "평균 제곱근 오차 RMSE (Root Mean Square Error) \\\n",
    "- RMSE(X,h)\n",
    "- 회귀 문제의 대표 성능 지표\n",
    "- 유클리디안 노름 Euclidean norm \\\n",
    "만약 이상치로 보이는 구역이 많다 -> 평균 절대 오차 MAE (Mean Absolute Error) 고려\n",
    "- MAE(X,h)\n",
    "- 절대값의 합을 계산\n",
    "- 맨해튼 노름 Manhattan norm\n",
    "\n",
    "### 2.3 가정 검사\n",
    "우리가 만든 모델이 다음 작업에서 어떻게 쓰이는지 확인해야 한다. \\\n",
    "예를들어 우리가 만든 모델의 결과가 회귀값으로 나타나는데 이를 활용한 다음 작업에서 결과값을 카테고리화하여 사용하려한다면 우리가 예측하게 되는 회귀값은 사용되지 않게된다. 이 같은 상황에서는 회귀값을 만들어내는 회귀 모델은 아무도 필요치 않으므로 분류값을 결과로 만드는 분류 시스템을 구축해야 한다. \\\n",
    "이처럼 우리가 만들어내는 모델이 어디서 어떻게 쓰이는지 파악한다면 불필요한 모델을 만드는 자원 낭비를 줄일 수 있을 것이다.\n",
    "\n",
    "### 3.1 작업환경 만들기\n",
    "- 해당 책에서는 파이썬을 이용하였다. \n",
    "- 파이썬을 이용해 Jupyter, Numpy, Pandas, Matplotlib, Scikit-learn 패키지를 사용할 수 있다. \n",
    "- pip를 이용해 패키지, 라이브러리를 다운로드받을 수 있다.\n",
    "- 가상환경 생성(다른 프로젝트의 라이브러리 버전과 충돌하는 것을 방지한다)\n",
    "\n",
    "### 3.3 데이터 구조 훑어보는 간단한 튜토리얼\n",
    "- head()\n",
    "- info() : 데이터 간단한 설명, 전체 행 수, 각 컬럼의 데이터 타입, 널값 갯수 확인\n",
    "- value_counts() : 특정 열의 데이터값의 종류와 각 종류가 얼마나 있는지 확인\n",
    "- describe() : 숫자형 특성의 요약 정보. 기술 통계량. null값을 제외하고 계산한다 \n",
    "    - std : 표준편차, 값이 퍼진 정도\n",
    "- hist() : 히스토그램 그래프. \n",
    "    - 데이터의 형태 확인가능, 스케일 여부 확인 가능\n",
    "    - 히스토그램의 꼬리 확인, 왜도 첨도 확인 가능\n",
    "    - 만약 꼬리가 두꺼운 분포라면, 데이터를 변형해야 할 것이다(로그 스케일로 ...)\n",
    "\n",
    "### 3.4 좋은 데이터 세트 만들기\n",
    "과대적합을 방지하기 위해서 질 좋은 데이터가 필요하다. 가지고 있는 데이터를 최대한 활용해야 한다. \\\n",
    "train 데이터에서만 성능이 좋게 나오고 test 데이터에서는 성능이 그만큼 좋게 나오지 않는것을 데이터 스누핑 편향이라고 한다. \\\n",
    "무작위로 샘플을 선택, dataset 중 20%를 test 데이터로 설정한다. \n",
    "\n",
    "방법 1. \\\n",
    "train, test split 함수 작성 \\\n",
    ": 하지만 이는 매번 train, test 데이터가 뒤바뀐다.\n",
    "\n",
    "방법 2. \\\n",
    "test data를 고정, 저장하여 불러와 사용한다.\n",
    "\n",
    "방법 3. \\\n",
    "`np.random.seed(42)` 난수 초기값을 먼저 설정 \\\n",
    "train, test split 함수 작성 \\\n",
    "난수 초기값을 고정시키므로 test 데이터를 고정실 수 있다.\n",
    "\n",
    "하지만 방법 1,2,3 모두 데이터셋이 추가되면 문제가 생긴다. \\\n",
    "데이터셋 업데이트에도 안정적인 분할이 가능해야한다. \\\n",
    "##### 해시값\n",
    "이를 위한 해결책으로 _샘플의 식별자_ 를 사용할 수 있다. \\\n",
    "각 데이터(샘플)마다 _식별자의 해시값_ 을 계산, 부여. 해시 최대값의 20%이하의 데이터만 test data로 보내는 방법이다. \\\n",
    "이 방법은 여러 번 반복 데이터셋 갱신돼도 test dataset이 동일하게 유지된다.  \\\n",
    "함수로 만들자면 아래와 같다.\n",
    "```python\n",
    "from zlib import crc32\n",
    "\n",
    "def test_set_check(identifier, test_ratio):\n",
    "    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32\n",
    "\n",
    "def split_train_test_by_id(data, test_ratio, id_column):\n",
    "    is = data[id_column]\n",
    "    in_test_set = ids.apply(lambda id_:test_set_check(id_, test_ratio))\n",
    "    return data.loc[~in_test_set], data.loc[in_test_set]\n",
    "\n",
    "data_add_id = data.rest_index() # 식별자 컬럼이 없으면 행의 인덱스를 ID로 사용\n",
    "train, test = split_train_test_by_id(data_add_id, 0.2, \"index)\n",
    "```\n",
    "이같이 _행의 인덱스_ 를 _고유 식별자_ 로 사용시 \n",
    "1. new data는 데이터셋의 가장 끝에 add되어야 하고,\n",
    "2. 어떤 행도 삭제되면 안된다.\n",
    "\n",
    "만약 위 두가지 조건이 불가능하다면 안정적인 특성으로 _고유 식별자_ 를 만들자. \\\n",
    "예를들어 위도와 경도같은 몇백 년후까지 믿고 쓸 수 있는 값으로 말이다.\n",
    "\n",
    "\n",
    "우리는 scikit-learn을 이용해 데이터를 나눌 수 있다. \\\n",
    "`train_test_split` \n",
    "- random_state 매개변수가 있다\n",
    "- 행의 개수가 같다면 여러 컬럼을 가진 데이터프레임도 인덱스 기반으로 나눌 수 있다.\n",
    "- 무작위로 샘플을 train, test로 나눌 수 있다. \\\n",
    "`train_test_split(data, test_size=0.2, random_state=42)`\n",
    "\n",
    "\n",
    "dataset이 충분히 크지 않다면 data sampling 편향이 생길 가능성이 크다. \\\n",
    "무작위로 데이터를 골랐을 때 여성 30%, 남성 70% 인 경우이다. 이를 계층적 샘플링이라고 한다. \\\n",
    "test data가 전체 dataset의 여러 카테고리를 잘 대표해야한다. dataset에 계층별로 충분한 sample 수가 있어야한다. \\\n",
    "if 그렇지 않으면 평향이 발생한다.\n",
    "- 너무 많은 계층으로 나누지 말 것\n",
    "- 각 계층의 데이터셋이 충분히 많아야한다.\n",
    "\n",
    "`pd.cut()'을 이용해 카테고리 특성을 만들 수 있다.\n",
    "```python\n",
    "housing['income_cat'] = pd.cut(housing['median_income'],\n",
    "                                bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                                labels=[1, 2, 3, 4, 5])\n",
    "```\n",
    "StratifiedShffleSplit를 사용하여 각 카테고리별 데이터를 전체 데이터셋과 비교하여 test 데이터에서 비율이 같게 뽑을 수 있다. \\\n",
    "```python\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing['income_cat]):\n",
    "    start_train_set = housing.loc[train_index]\n",
    "    start_test_set = housing.loc[test_index]\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4.1 지리적 데이터 시각화\n",
    "**산점도** \\\n",
    "`data.plot(kind=\"scatter\", x=\"경도\", y=\"위도\")` \\\n",
    "여기에 alpha 매개변수로 점의 흐릿한 정도를 조절할 수 있다 -> 밀집된 영역 파악하기 좋다 \\\n",
    "기타 다른 매개변수 옵션을 주고 싶다면 matplotlib 속성을 찾아보면 되겠다.\n",
    "\n",
    "### 4.2 상관관계 조사\n",
    "방법 1. 표준 상관계수(피어슨Pearson) \\\n",
    "`data.corr()` corr() 메서드를 이용해 쉽게 계산 가능하다.\n",
    "\n",
    "평가기준\n",
    "* 상관관계 범위 : -1 ~ 1\n",
    "* 1에 가깝다 = 강한 양의 상관관계\n",
    "* -1에 가깝다 = 강한 음의 상관관계\n",
    "* 0에 가깝다 = 선형적인 상관관계가 없다\n",
    "\n",
    "주의점 : 상관계수는 선형적인 상관관계만 측정. 비선형적 관계는 알 수 없다.\n",
    "\n",
    "방법 2. 산점도 그려보기 \\\n",
    "`scatter_matrix()` 함수사용\n",
    "\n",
    "머신러닝 알고리즘용 데이터셋을 준비하는 마지막 시도는  '여러 특성의 조합'을 만들어보는 것이다. \\\n",
    "기존의 데이터를 조합해 새로운 컬럼을 만들어본다. \\\n",
    "새로만든 컬럼을 포함시켜 상관관계를 재조사해볼 수 있겠다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 데이터 정제\n",
    "`data.dropna()` data의 null값이 존재하는 모든 행을 삭제한다. subset 매개변수를 주어 특정 컬럼을 지정할 수 있다. \\\n",
    "`data.drop('col1', axis=1)` col1 컬럼을 삭제하는데 axis 1을 기준으로한다. \\\n",
    "`data['col2'].fillna(data['col2'].median(), inplace=True)` col2에서 NaN값인 부분을 col2의 median값으로 대체하며 inplace=True를 하면 새롭게 변수로 선언해주지 않아도 data['col2']바로 저장해준다.\n",
    "\n",
    "**누락된 값 다루기**\n",
    "median값으로 대체해보자\n",
    "\n",
    "방법 1. SimpleImputer 메소드 사용 \\\n",
    "이때 수치형 데이터에만 적용해보겠다.\n",
    "```\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "imputer.fit(data_num)\n",
    "```\n",
    "이때 statistics_ 속성에 각 특성의 중앙값이 저장되므로\n",
    "```\n",
    "imputer.statistics_\n",
    "```\n",
    "를 출력하면 각 수치형 컬럼의 중앙값을 return받을 수 있다. \n",
    "\n",
    "**Scikit-learn 설계**\n",
    "우리는 Scikit-learn API로 간편한 인터페이스를 사용할 수 있다. 궁금했던 점들을 이해하며 적어보자.\n",
    "\n",
    "추정기 estimator\n",
    "- fit() method로 수행된다\n",
    "- 하나의 매개변수로 하나의 데이터셋만 전달\n",
    "\n",
    "변환기 transformer\n",
    "- 데이터셋을 변환하는 추정기\n",
    "- 변환하는 법 : transform() 메서드 수행후 변환된 데이터셋 반환\n",
    "- fit(), transform()을 연달아 호출 or fit_transform() 메서드 소유 \n",
    "- fit(), transform() 연달아 호출하는 속도보다 fit_transform() 가 최적화되어있어 더 빠를때가 있다\n",
    "\n",
    "예측기 predictor\n",
    "- 새로운 데이터셋을 input받아 예측값을 return한다\n",
    "- test datset을 이용해 예측 품질도를 측정할 수 있다. score() 메서드 사용.\n",
    "\n",
    "검사 가능\n",
    "- 모든 추정기의 하이퍼파라미터 공개 인스턴스 변수로 접근 가능\n",
    "- 모든 추정기의 학습된 모델 파라미터는 접미사 _ 밑줄을 붙여 공개 인스턴스 변수로 제공됨\n",
    "- 예) imputer.statistics_\n",
    "\n",
    "클래스 남용 방지 \n",
    "- 데이터셋을 numpy array, scipi spare matrix로 표현\n",
    "- 하이퍼파라미터는 보통 파이썬 string or number type\n",
    "\n",
    "조합성\n",
    "- 기존의 구성요소 최대한 재사용\n",
    "- 여러 변환기 + (마지막)추정기 하나 -> Pipeline 추정기 생성\n",
    "\n",
    "합리적인 기본값\n",
    "- 대부분의 매개변수에 합리적인 기본값이 지정되어있음\n",
    "\n",
    "---\n",
    "\n",
    "### 5.2 Text, Category 특성 다루기\n",
    "대부분의 머신러닝 알고리즘은 숫자를 다룬다. \\\n",
    "그러므로 범주형 텍스트 특성은 숫자로 변환해야한다.\n",
    "\n",
    "방법1. scikit-learn의 OrdinalEncoder Class \\\n",
    "- 카테고리별로 0, 1, 2, 3, ... 와 같이 숫자를 부여해준다. \\\n",
    "- `OrdinalEncoder().catogories_` 를 이용해 카테고리 목록을 볼 수 있다. \n",
    "- 순서형 카테고리(학점(A,B,C), 영화평점)에 적합\n",
    "\n",
    "순서형 카테고리가 아닌 경우 이진 특성을 만들어 해결할 수 있다.\n",
    "\n",
    "방법2. OneHotEncoder Class 사용 \\\n",
    "- 각 카테고리마다 True, False 값으로 반환하므로 카테고리수만큼 컬럼의 값도 늘어나게된다.\n",
    "- 0의 값이 많게되는데 0값을 모두 저장하는 것은 낭비이므로 -> 희소 행렬 Compressed Sparse로 저장한다. (0이 아닌 원소의 위치를 저장하는 방식)\n",
    "- 희소행렬은 1. 2차원 배열처럼 사용가능 2. 넘파이 배열로 쓰려면 .toarray() 메소드를 쓰면된다.\n",
    "- But, 카테고리수가 too many 하다면 훈련을 느리게 하고 성능이 감소된다\n",
    "\n",
    "---\n",
    "\n",
    "### 5.3 커스텀 변환기\n",
    "duck typing : 상속, 인터페이스 방식의 구현이 아닌, 객체의 속성, 메서드가 객체의 유형을 결정하는 방식\n",
    "\n",
    "scikit-learn은 duck typing을 지원하므로 pipeline 같은 기능으로 변환기를 연동할 때 -> fit(), transform(), fit_transform() 메서드가 사용가능한 class를 구현하면 된다. \\\n",
    "이는 fit() ... 메서드를 가진 클래스여야한다는 것인데, \\\n",
    "`TransformerMixin`을 상속하면 fit_transform() 메서드를 사용할 수 있게 된다. \\\n",
    "여기서 duck typing이 상속은 아니지만 메서드를 가진 클래스를 이용해(상속해) 메서드를 사용할 수 있다는 것이 포인트인것 같다. \\\n",
    "`BaseEstimator`를 상속하면 하이퍼파라미터 튜닝에 필요한 get_params(), set_params() 두 메서드를 사용할 수 있다.\n",
    "\n",
    "아래는 이를 이용한 변환기의 예시이다.\n",
    "```python\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init(self, add_bedrooms_per_room=True): \n",
    "        \"\"\"add_bedrooms_per_room라는 하이퍼파라미터깞 하나를 가진다. 기본값 True로 설정해주었다.\"\"\"\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, X, y=None):\n",
    "        return self \n",
    "    \"\"\"TransformerMixin을 상속함으로써 fit, transform 을 사용할 수 있다.\"\"\"\n",
    "    def transform(self, X) :\n",
    "        rooms_per_household = X[:, rooms_ix]/X[:, households_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix]/X[:,rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "\n",
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
    "housing_extra_attribs=attr_adder.transform(housing.values)\n",
    "```\n",
    "만들어본 변화기에는 시도해보고 싶은 하이퍼파라미터값을 쉽게 넣어 확인해볼 수 있다. 이를 통해 최상의 조합을 자동화하여 찾을 수 있다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### 5.4 특성 스케일링\n",
    "1. min-max 스케일링 normalization\n",
    "(data - mean)/표준편차 \\\n",
    "0 ~ 1 사이의 범위로 값을 변경한다 \\\n",
    "`MinMaxScaler` 사용하여 구현\n",
    "\n",
    "2. 표준화 standardization\n",
    "(data-min)/(max-min) \\\n",
    "표준화한 데이터들은 평균이 0, 분포의 분산이 1이 된다 \\\n",
    "이상치의 영향을 덜 받는다 \\\n",
    "값의 상한과 하한이 없다는 점은 문제가 될 수 있다 \\\n",
    "`StandardScaler` 변화기로 구현\n",
    "\n",
    "---\n",
    "\n",
    "### 5.5 변환 파이프라인\n",
    "Scikit-learn 을 이용한 pipeline 구현방법 - Pipeline Class 사용하기\n",
    "\n",
    "아래는 수치형 데이터를 상대로한 pipeline이다.\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = PipeLine([\n",
    "    # tuple 형태로 (변환기지정이름, 변환기클래스) 적어준다.\n",
    "    # 변환기\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    ('attribs_adder', CombinedAttributesAdder()),\n",
    "    # 변환기와 추정기\n",
    "    ('std_scaler', STandardScaler()),\n",
    "])\n",
    "```\n",
    "\n",
    "`ColumnTransformer` 클래스를 이용하여 각 컬럼마다 적합한 변환기를 적용하여 모든 데이터를 한 번에 처리할 수 있다.\n",
    "아래는 수치형과 범주형 데이터를 따로 두었다.\n",
    "```python\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_att = list(num_data)\n",
    "cat_att = [\"species\"]\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_att),\n",
    "    (\"cat\". OneHotEncoder(), cat_att),\n",
    "])\n",
    "\n",
    "data_trim = full_pipeline.fit_transform(data)\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "! 여기서\n",
    "ColumnTransformer는 만약 희소행렬과 밀집행렬이 섞여있다면 -> 최종 행렬의 밀집 정도를 추정한다(0이 아닌 원소의 비율) -> 밀집 정도가 임계값(sparse_threshold=0.3이 디폴트)보다 낮으면 희소 행렬을 return\n",
    "\n",
    "OneHotEncoder()는 희소 행렬을 반환하지만 위 예에서는 num_pipeline은 **밀집 행렬** 을 반환한다!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 train dataset을 train and 평가\n",
    "\n",
    "선형 회귀 모델을 이용하는 과정\n",
    "1. 선형 회귀 모델 선언\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(data_trim, data_labels)\n",
    "```\n",
    "2. 몇 개의 샘플 train data에 적용\n",
    "```python\n",
    "some_data_trim = full_pipeline.transform(some_data)\n",
    "y_hat = lin_reg.predict(some_data_trim)\n",
    "```\n",
    "여기서 y_hat과 실제 y값(data_labels)를 출려하여 눈으로 확인할 수도 있다.\n",
    "\n",
    "3. 평가 지표로 성능 확인\n",
    "임의로 RMSE로 측정\n",
    "```python\n",
    "from sklearn.metrics import mean_squared_error\n",
    "y_hat=lin_reg.predict(data_trim)\n",
    "lin_mse=mean_squared_error(data_labels, y_hat)\n",
    "lin_rmse=np.sqrt(lin_mse)\n",
    "print(\"RMSE=\",lin_rmse)\n",
    "```\n",
    "y_hat과 data_labels의 값의 차이가 난다면, RMSE의 값이 크게 나온다면 과소적합의 경우로 판단될 수 있다. \\\n",
    "과소적합의 경우 데이터가 더 필요하거나 모델선정에서 모델이 강력하지 못하다는 것으로, \\\n",
    "    1. 강력한 모델을 선정하거나 2. train data에 더 좋은 컬럼을 추가하거나 3. 모델의 규제를 감소시키면된다.\n",
    "\n",
    "만약 RMSE의 값이 0.0으로 완벽한 모델링에 성공한다면 과대적합을 의심해봐야한다. \\\n",
    "train, test data로 데이터셋을 나누어 훈련을 시도해야한다.\n",
    "\n",
    "\n",
    "### 6.2 교차 검증을 사용하여 평가해보자\n",
    "scikit-learn에 k-겹 교차검증 기능을 사용할 수 있다.\n",
    "- `from sklearn.model_selection import cross_val_score`를 사용하면 된다.\n",
    "- MSE의 반댓값, neg_mean_squared_error를 사용한다 == 값이 클수록 좋다는 의미인 효용 함수를 기대한다(MSE는 비용함수 기대)\n",
    "- 데이터셋을 여러번 검증하므로 비용이 많이든다.\n",
    "\n",
    "이 책에서는 하이퍼파라미터값을 많이 조정하는 작업을 하는 대신 다양한 종류의 모델을 시도해봐야한다고 한다. \\\n",
    "그 중 가능성있는 2 ~ 5개의 모델을 선정한다.\n",
    "\n",
    "> 팁 : python의 **pickle package** or **joblib**을 이용해 __scikit-learn__ 모델을 저장할 수 있다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 세부 튜닝\n",
    "### 7.1 그리드 탐색\n",
    "Simple -> 하이퍼파라미터값을 수동으로 조정하여 최고의 조합을 찾는다. \\\n",
    "많은 하이퍼파라미터값 조합 경우의 수 -> 경우의 수를 자동으로 탐색해주는 것 == 그리드 탐색 GridSearchCV\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid=[\n",
    "    {'n_estimators':[3, 10, 30], 'max_features':{2,4,6,8}},\n",
    "    {'bootstrap':[False], 'n_estimators':[10, 30], 'max_features':{4,6,8}},\n",
    "]\n",
    "\n",
    "randfor = RandomForestRegressor()\n",
    "grid_search = GridSearchCV(randfor,\n",
    "                            param_grid,\n",
    "                            cv=5,\n",
    "                            scoring='neg_mean_squared_error',\n",
    "                            return_train_score=True)\n",
    "grid_search.fit(data_trim, data_labels)\n",
    "```\n",
    "\n",
    "param_grid 안의 경우들을 자동으로 모두 시도해보고 가장 성능이 좋은 경우를 알려준다.\n",
    "\n",
    "```python\n",
    "grid_result = grid_search.cv_results_\n",
    "for mean_score, params in zip(grid_result[\"mean_test_score\"], grid_result[\"params\"]):\n",
    "    # mean_score는 효용 함수이므로 음수를 붙여준다. MSE 기준(비용 함수)로 보기위함\n",
    "    print(np.sqrt(-mean_score), params)\n",
    "```\n",
    "\n",
    "\n",
    "### 7.2 랜덤 탐색\n",
    "그리드 탐색 -> 적은 수의 조합에서 Good \\\n",
    "But, 탐색할게 커진다 -> `RandomizedSearchCV` 를 써라 \n",
    "\n",
    "\n",
    "### 7.3 앙상블 방법\n",
    "때론 _모델 그룹 성능 > 최상의 단일 모델 성능_ 일때가 있다. \\\n",
    "각 모델이 다른 형태의 오차를 만든다면 더더욱.\n",
    "\n",
    "### 7.4 최상의 모델과 오차 분석\n",
    "모델을 분석하며 데이터셋에 대한 분석이 가능합니다. \\\n",
    "예) RandomForestRegressor 는 각 feature의 상대적 중요도 출력 -> 어떤 feature가 덜 줄요한지 알 수 있다.\n",
    "\n",
    "```python\n",
    "important_f = grid_search.best_estimator_.feature_importances_\n",
    "``` \n",
    "important_f 를 이용하게된다.\n",
    "\n",
    "덜 중요한 feature는 제외해도되고, 비교했을 때 좀 다른 오차가 발생된걸 보면 그 이유를 생각해보며 오차를 줄일 방법을 생각하면된다. \\\n",
    "앞서 말했던 feature를 추가생성하거나 불필요한것을 제외하거나 이상치를 제거하는 등의 작업들 말이다.\n",
    "\n",
    "### 7.5 test data로 결과보기\n",
    "일반화 오차의 95% 신뢰구간을 계산\n",
    "- 우리가 만든 모델의 성능을 더 정확히 알고싶다면\n",
    "- `scipy.stats.t.interval()`을 이용한다.\n",
    "\n",
    "if 하이퍼파라미터 튜닝을 많이 시도 :\n",
    "    교차 검증을 사용해 측정한 값 >= 보다 성능이 낮게 나올 수 있다. \n",
    "\n",
    "자! 이제 론칭을 할 준비가 끝나간다...\n",
    "---\n",
    "솔루션과 문서, 깔끔한 도표와 기억하기 쉬운 제목이 적힌 자료로 발표를 한다.\n",
    "\n",
    "\n",
    "## 드디어 론칭...!\n",
    "론칭을 허가받는다면 앞으로도 단계적으로 해야할일이 있다.\n",
    "1. 코드 정리, 문서와 테스트 케이스 작성\n",
    "2. 상용 환경에 모델 배포\n",
    "\n",
    "- 잠시 모델을 배포하려면... \n",
    "\n",
    "    - 방법1. (전처리 + 예측 파이프라인) 만든 모델을 joblib 같은거로 저장한다.\n",
    "    1. 저장한 모델을 상용 환경에서 load (서버가 시작될때 load하는게 좋다)\n",
    "    2. 데이터가 담긴 query가 웹서버로 전송 -> 웹 앱으로 전달 -> 앱의 코드가 모델의 predict()를 호출 -> y_hat 값을 만든다\n",
    "    \n",
    "    \n",
    "    - 방법2. REST API 웹 서비스 방법 : 모델을 새 버전으로 업데이트하기 쉽다, 질의시 로드 밸런싱이 가능하다. 규모 확장이 쉽다. 다양한 언어로 작성이 가능하다.\n",
    "\n",
    "    - 방법3. AI 플랫폼인 클라우드에 배포 : 모델 저장 -> 클라우드 스토리지에 업로드 -> AI 플랫폼으로 이도해 새 모델 버전을 만들고 파일을 지정, \n",
    "    로드밸런싱과 자동 확장 처리용 간단한 웹 서비스를 만들어주면 input data담은 JSON 요청을 받고 y_hat을 담은 JSON 응답을 return \n",
    "    \n",
    "3. 실시간 성능을 체크 (모니터링 코드 작성), 컴포넌트의 고장으로 인한 급격한 성능 감소 혹은 완만한 성능 감소를 감시해야 함 -> 주기적 재훈련의 필요성\n",
    "4. 모델 실패시 무엇을 할지, 어떻게 대비할지 ~ 프로세스 준비\n",
    "5. 데이터가 변화한다 = 데이터셋을 업데이트 + 주기적 재훈련\n",
    "\n",
    "이 모든 과정에서 가능한 많은 부분을 자동화해야 좋다...! 아래는 자동화할 수 있는 목록이다.\n",
    "- 정기적 데이터셋 update & labeling\n",
    "- model traiing & 하이퍼파라미터를 자동으로 세부 튜닝\n",
    "- update한 model과 이전 버전의 model 을 비교하는 작업\n",
    "\n",
    "그리고, + input data 품질 관리(평균, 표준편차가 train set과 많이 달라지거나(멀어지거나) 새로운 label값이 등장하는 경우)\n",
    "\n",
    "\n",
    "6. 모델 백업\n",
    "언제라도 이전 버전의 모델을 사용할 수 있어야함. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (v3.10.6:9c7b4bd164, Aug  1 2022, 17:13:48) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
