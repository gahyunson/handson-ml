{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 텐서플로\n",
    "\n",
    "- 강력한 수치 계산용 라이브러리\n",
    "- 구글 브레인 팀에서 개발\n",
    "- 구글 클라우드 스피치, 구글 포토, 구글 검색 대규모 서비스에 사용됨\n",
    "- 2015년 11월 오픈 소스 공개\n",
    "- GPU 지원\n",
    "- 분산 컴퓨팅 지원\n",
    "- Just-In-Time 컴파일러 포함 - 소도를 높이고 메모리 사용량을 즐이기 위해 계산을 최적화함\n",
    "- 다른 환경에서 훈련과 실행이 가능하다\n",
    "- 자동 미분 기능과 고성능 옵티마이저 RMSProp, Nadam 제공\n",
    "- TensorFlow.js 자바스크립트 구현 있음, 브라우저에서 직접 모델 실행 가능한 것\n",
    "\n",
    "저수준 텐서플로 연산 \n",
    "- C++ 코드로 구현\n",
    "- 여러 구현(kernel)이 존재 \n",
    "- kernel은 CPU, GPU, TPU 같은 특정 장치에 맞춰 만들어짐\n",
    "\n",
    "[TensorFlow GitHub](https://github.com/jtoy/awesome-tensorflow) 에서 많은 신경망 구조 다운로드 가능\n",
    "\n",
    "TFX : TensorFlow 제품화를 위한 라이브러리 모음\n",
    "\n",
    "[TensorFlow 버그 알리기, 새로운 기능 요청하기](https://github.com/tensorflow/tensorflow)    \n",
    "[TensorFlow 커뮤니티](https://homl.info/41)\n",
    "\n",
    "# 2. Using the TensorFlow LIKE NUMPY\n",
    "\n",
    "##### tensor\n",
    "- 한 연산에서 다른 연산으로 ... ~ tensorflow\n",
    "- numpy - ndarray와 매우 비슷함\n",
    "- 다차원 배열, 스칼라 값 가능    \n",
    "텐서 생성과 조작\n",
    "\n",
    "## 2.1 텐서와 연산\n",
    "tf.constant()\n",
    "\n",
    "1. 행렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-09 20:51:44.572241: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-09 20:52:39.707103: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.constant([[1., 2., 3.], [4., 5., 6.]]) # matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 스칼라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=42>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(42) # scalar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 크기와 데이터 타입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       " array([[1., 2., 3.],\n",
       "        [4., 5., 6.]], dtype=float32)>,\n",
       " TensorShape([2, 3]),\n",
       " tf.float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n",
    "t, t.shape, t.dtype"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Tensor and Numpy\n",
    "\n",
    "넘파이 배열에 텐서플로 연산 가능    \n",
    "텐서에 넘파이 연산 가능\n",
    "\n",
    "## 2.3 타입 변환\n",
    "타입 자동 변환 안함    \n",
    "호환되지 않는 타입의 텐서로 연산시 (실수 텐서 * 정수 텐서) 예외 발생    \n",
    "(32bit 실수 * 64bit 실수)    \n",
    "변환 필요시 `tf.cast()` 함수 사용\n",
    "\n",
    "\n",
    "## 2.4 변수\n",
    "텐서의 내용은 수정 불가 = 역전파시 신경망의 가중치를 변경할 수 없음\n",
    "-> tf.Variable 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = tf.Variable([[1., 2., 3.], [4., 5., 6.]])\n",
    "v"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tf.Variable은 tf.Tensor와 비슷하게 작동함\n",
    "- assign() 이나 scatter_update(), scatter_nd_update() 로 개별 원소를 수정 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
      "array([[ 2.,  4.,  6.],\n",
      "       [ 8., 10., 12.]], dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
      "array([[100.,   4.,   6.],\n",
      "       [  8.,  10., 200.]], dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
      "array([[4., 5., 6.],\n",
      "       [1., 2., 3.]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "v.assign(2 * v)\n",
    "print(v)\n",
    "\n",
    "v.scatter_nd_update(indices=[[0, 0], [1, 2]],\n",
    "                    updates=[100., 200.])\n",
    "print(v)\n",
    "\n",
    "sparse_delta = tf.IndexedSlices(values=[[1., 2., 3.], [4., 5., 6.]],\n",
    "                                indices=[1, 0])\n",
    "v.scatter_update(sparse_delta)\n",
    "print(v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 데이터 구조\n",
    "- 희소 텐서\n",
    "- 텐서 배열 : 텐서의 리스트. 리스트에 포함된 모든 텐서 크기와 데이터 타입은 동일해야함\n",
    "- 래그드 텐서 : 리스트의 리스트. 리스트의 길이는 다를 수 있음. tf.ragged package\n",
    "- 문자열 텐서 : tf.string . 바이트 문자열을 나타냄(유니코드 아님!) 자동으로 UTF-8로 인코딩\n",
    "- 집합 : 텐서 혹은 희소텐서로 표현. tf.sets package\n",
    "- 큐 : tf.queue \n",
    "\n",
    "# 3. 사용자 정의 모델과 훈련 알고리즘\n",
    "\n",
    "## 3.1 사용자 정의 손실 함수\n",
    "train dataset noise -> 이상치 제거 혹은 데이터셋 수정 후에도 noise가 남아 있을거라고 가정    \n",
    "이러한 경우 사용할 수 있는 손실 함수?\n",
    "\n",
    "- 평균 제곱 오차 : 큰 오차에 큰 패널티가 주어지기 때문에 정확한 모델이 안만들어짐\n",
    "- 평균 절댓값 오차 : 이상치에 관대, 수렴되는데 오래걸림\n",
    "- 후버 손실 : L1과 L2의 장점과 단점을 보완, 모든 지점에서 미분이 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_fn(y_true, y_pred):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < 1\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    linear_loss = 1 * tf.abs(error) - 1 / 2\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "\n",
    "model.compile(loss=huber_fn, optimizer=\"nadam\")\n",
    "model.fit(X_train, y_train, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-24 21:53:10.737342: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'huber_fn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/gahyunson/Work/Handson_ml/12_tensorflow_training.ipynb 셀 13\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gahyunson/Work/Handson_ml/12_tensorflow_training.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/gahyunson/Work/Handson_ml/12_tensorflow_training.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m huber_fn(\u001b[39m0.98\u001b[39m, \u001b[39m0.70\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'huber_fn' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "huber_fn(0.98, 0.70)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Custom 요소를 넣은 Model Save & Load\n",
    "\n",
    "1. Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_model.h5\", \n",
    "                                custom_objects={\"huber_fn\":huber_fn})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "상단의 후버 손실은 -1 ~ 1 사이의 오차는 작은것으로 간주됐다.    \n",
    "만약 custom한 오차 함수에 다른 기준을 부여하려면 매개 변수를 받을 수 있는 함수를 만든다.    \n",
    "threshold 값은 저장되지 않으므로, model load시 값을 지정해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_huber(threshold=1.0):\n",
    "    def huber_fn(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss  = threshold * tf.abs(error) - threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    return huber_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/gahyunson/Work/Handson_ml/12_tensorflow_training.ipynb 셀 18\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/gahyunson/Work/Handson_ml/12_tensorflow_training.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39mcreate_huber(\u001b[39m2.0\u001b[39m), optimizer\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnadam\u001b[39m\u001b[39m\"\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mmae\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gahyunson/Work/Handson_ml/12_tensorflow_training.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39mfit(X_train_scaled, y_train, epochs\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gahyunson/Work/Handson_ml/12_tensorflow_training.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m           validation_data\u001b[39m=\u001b[39m(X_valid_scaled, y_valid))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gahyunson/Work/Handson_ml/12_tensorflow_training.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mmy_model_with_a_custom_loss_threshold_2.h5\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.compile(loss=create_huber(2.0), optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "model.save(\"my_model_with_a_custom_loss_threshold_2.h5\")\n",
    "\n",
    "# 매개변수를 이용한 custom 함수 조정\n",
    "model = keras.models.load_model(\"my_model_with_a_custom_loss_threshold_2.h5\",\n",
    "                                custom_objects={\"huber_fn\": create_huber(2.0)})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "상단의 코드를 클래스 상속을 이용하여 구현할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberLoss(keras.losses.Loss):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss  = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}\n",
    "    \n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "model.compile(loss=HuberLoss(2.), optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "model.save(\"my_model_with_a_custom_loss_class.h5\")\n",
    "\n",
    "\n",
    "model = keras.models.load_model(\"my_model_with_a_custom_loss_class.h5\",\n",
    "                                custom_objects={\"HuberLoss\": HuberLoss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "\n",
    "model.loss.threshold # thresshold값 확인 가능"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 활성화 함수, 초기화, 규제, 제한 Customizing\n",
    "\n",
    "입, 출력을 가진 간단한 함수로 작성하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 활성화 함수\n",
    "def my_softplus(z): # return value is just tf.nn.softplus(z)\n",
    "    return tf.math.log(tf.exp(z) + 1.0)\n",
    "\n",
    "# 글로럿 초기화\n",
    "def my_glorot_initializer(shape, dtype=tf.float32):\n",
    "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "\n",
    "# 사용자 정의 l1 규제\n",
    "def my_l1_regularizer(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
    "\n",
    "# 양수인 가중치만 남기는 사용자 정의 제한\n",
    "def my_positive_weights(weights): # return value is just tf.nn.relu(weights)\n",
    "    return tf.where(weights < 0., tf.zeros_like(weights), weights)\n",
    "\n",
    "# 레이어에 사용자 정의 함수들을 선언해준다\n",
    "layer = keras.layers.Dense(1, activation=my_softplus,\n",
    "                           kernel_initializer=my_glorot_initializer,\n",
    "                           kernel_regularizer=my_l1_regularizer,\n",
    "                           kernel_constraint=my_positive_weights)\n",
    "    \"\"\"\n",
    "    활성화 함수 -> Dense 층 출력에 적용됨 -> 그 다음 층에 결과 전달\n",
    "    가중치 : 초기화 함수 return값으로 초기화됨\n",
    "    1st step -> 가중치가 규제 함수에 전달 -> 규제 손실 계산 -> 규제 손실값 전체 손실에 추가 \n",
    "    -> 최종 손실 형성 -> 2nd step\n",
    "    1st step -> 제한 함수 호출 -> 가중치를 제한한 가중치 값으로 변경 -> 2nd step\n",
    "    \"\"\"\n",
    "\n",
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1, activation=my_softplus,\n",
    "                       kernel_regularizer=my_l1_regularizer,\n",
    "                       kernel_constraint=my_positive_weights,\n",
    "                       kernel_initializer=my_glorot_initializer),\n",
    "])\n",
    "\n",
    "# ... model fit ...생략\n",
    "model.save(\"my_model_with_many_custom_parts.h5\")\n",
    "\n",
    "model = keras.models.load_model(\n",
    "    \"my_model_with_many_custom_parts.h5\",\n",
    "    custom_objects={\n",
    "       \"my_l1_regularizer\": my_l1_regularizer,\n",
    "       \"my_positive_weights\": my_positive_weights,\n",
    "       \"my_glorot_initializer\": my_glorot_initializer,\n",
    "       \"my_softplus\": my_softplus,\n",
    "    })"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "함수가 모델과 함께 저장되어야 하는 하이퍼파라미터를 같이 가지고 있다면...    \n",
    "손실, 활성화 함수 포함한 층 모델 - call() 메서드 있어야함    \n",
    "규제, 초기화, 제한 모델은 - __call__() 메서드 있어야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyL1Regularizer(keras.regularizers.Regularizer):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "    def __call__(self, weights):\n",
    "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
    "    def get_config(self):\n",
    "        return {\"factor\": self.factor}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 사용자 정의 지표\n",
    "\n",
    "손실 :     \n",
    "(cross entropy의 경우 ...) 훈련하려면 경사 하강법 사용 -> (평가할 영역) 미분 가능해야함 & gradient != 0 이어야 함 \n",
    "\n",
    "지표 :     \n",
    "(accuracy) 모델 평가 -> 미분 불가 or gradient != 0 이어도 ㄱㅊ  \n",
    "\n",
    "Usually, 사용자 지표 함수 만드는 것 = 사용자 손실 함수 만드는 것\n",
    "\n",
    "정밀도 = 예측 양성 / 찐 양성     \n",
    "ex) 1st predict : 4 / 5 = 80%,     \n",
    "2nd predict(잘못된 3개 양성 예측인 경우) : 0 / 5 = 0%     \n",
    "1st and 2nd mean : 40% -> This is NOT REAL 정밀도\n",
    "\n",
    "이처럼 배치마다 업데이트 되는 것 -> 스트리밍 지표\n",
    "\n",
    "예측한 것 중 진짜 양성 (4 + 0) / 양성 예측 (5+3) = 50%\n",
    "-> Precision\n",
    "\n",
    "아래는 Precision 구현한 코드 keras 이용할 수 있음 \n",
    "\n",
    "```\n",
    "precision = keras.metrics.Pricision()\n",
    "precision(y, y_hat)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision.result() # 현재 지표값 얻기\n",
    "precision.variables # TP, FP 변수 확인 \n",
    "precision.reset_states() # 위 변수 초기화"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스트리밍 지표를 만들려면 `keras.metrics.Metric` 클래스를 상속하면 된다.    \n",
    "\n",
    "아래는 전체 후버 손실 & 처리한 샘플 수 record - class 코드    \n",
    "return 평균 후버 손실    \n",
    "(아래 코드에서 keras가 지속적으로 변수를 관리하므로 따로 더 작성할 필요가 없는거다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberMetric(keras.metrics.Metric):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        super().__init__(**kwargs) # handles base args (e.g., dtype)\n",
    "        self.threshold = threshold\n",
    "        self.huber_fn = create_huber(threshold)\n",
    "        # add_weight : 여러 지표 추가\n",
    "        self.total = self.add_weight(\"total\", initializer=\"zeros\") # 후버 손실 합\n",
    "        self.count = self.add_weight(\"count\", initializer=\"zeros\") # 처리한 총 샘플 수\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None): # 변수 업데이트, HuberMetric class를 함수로 사용할 때 호출 \n",
    "        metric = self.huber_fn(y_true, y_pred)\n",
    "        self.total.assign_add(tf.reduce_sum(metric))\n",
    "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
    "    def result(self): # 최종 결과 계산 return\n",
    "        return self.total / self.count\n",
    "    def get_config(self): # threshold , model 저장\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 사용자 정의 층\n",
    "\n",
    "특이한 층을 가진 네트워크 - 사용자 정의 층 \n",
    "\n",
    "1. 가중치가 필요 없는 사용자 정의 층\n",
    "keras.layers.Lambda 로 파이썬 함수 감싸기\n",
    "\n",
    "(keras.layers.Flatten, keras.layers.RELU - 가중치가 없는 만들어져있는 층)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exponential_layer = keras.layers.Lambda(lambda x: tf.exp(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exponential_layer를 사용할 곳은 여러군데다.    \n",
    "[sequential API, 함수형 API, 서브클래싱 API, 활성화 함수]\n",
    "\n",
    "2. 가중치가 필요한 사용자 정의 층 keras.layers.Layer 상속해야 함\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
